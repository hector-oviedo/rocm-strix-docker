# ROCm Strix Docker - Complete Reference

Docker infrastructure for AMD Strix Halo (RDNA 3.5 / gfx1151) with ROCm PyTorch and Ollama LLM support.

## Problem Solved

Standard containers fail on Strix Halo because:
1. Missing gfx1151 support in stable ROCm
2. Ubuntu Rolling ships Python 3.13 (PyTorch doesn't support it)
3. Debian pip silently falls back to CPU wheels

Solution: Ubuntu Rolling (driver compatibility) + UV (Python 3.12 isolation) + ROCm preview wheels.

## Version Clarification

- **PyTorch/ROCm Package**: 7.11.0rc1 (latest TheRock-based preview)
- **HIP Runtime**: 7.2.x (lower-level driver interface, shown in `torch.version.hip`)
- **torch.__version__**: 2.9.1+rocm7.11.0rc1

These are DIFFERENT metrics. 7.11 is the actual ROCm stack version.

## Project Structure

```
.
├── .env                      # Environment variables (VIDEO_GID, RENDER_GID)
├── .env.template             # Template for environment variables
├── .gitignore                # Git ignore rules (excludes .env)
├── Dockerfile                # Base ROCm PyTorch container
├── docker-compose.yml        # Docker Compose configuration
├── entrypoint.sh             # Base container entrypoint
├── llm.txt                   # This complete reference
├── ollama/                   # Ollama LLM service
│   ├── Dockerfile            # Ollama ROCm container
│   └── entrypoint.sh         # Ollama auto-model loader entrypoint
├── README.md                 # User documentation
└── workspace/                # Mounted volume for persistent data
```

## Required Environment Variables

MUST be numeric GIDs, NOT group names:
```
VIDEO_GID=44      # From: getent group video | cut -d: -f3
RENDER_GID=991    # From: getent group render | cut -d: -f3
```

String group names (video, render) DO NOT WORK in Docker Compose group_add.

## Files

### .gitignore
```
.env
```

### .env.template
```
VIDEO_GID=
RENDER_GID=
```

### Dockerfile (Base Container)
```dockerfile
FROM ubuntu:rolling

ENV DEBIAN_FRONTEND=noninteractive
ENV UV_CACHE_DIR=/root/.cache/uv
ENV VIRTUAL_ENV=/app/.venv
ENV PATH="$VIRTUAL_ENV/bin:$PATH"
ENV HSA_OVERRIDE_GFX_VERSION=11.5.1

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip \
    libgl1 libglib2.0-0 libgomp1 \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

WORKDIR /app

RUN uv venv .venv --python 3.12 && \
    uv pip install --pre \
    torch torchvision torchaudio \
    --index-url https://rocm.prereleases.amd.com/whl/gfx1151/

COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]
CMD ["python", "-c", "import time; print('ROCm container ready. Use docker exec to run commands.'); time.sleep(86400)"]
```

### docker-compose.yml
```yaml
services:
  rocm-strix:
    build: .
    image: rocm-strix:latest
    container_name: rocm-strix
    privileged: true
    restart: unless-stopped
    devices:
      - "/dev/kfd:/dev/kfd"
      - "/dev/dri:/dev/dri"
    group_add:
      - "${VIDEO_GID}"
      - "${RENDER_GID}"
    volumes:
      - ./workspace:/workspace
    environment:
      - HSA_OVERRIDE_GFX_VERSION=11.5.1
      - HIP_VISIBLE_DEVICES=0
    ipc: host
    security_opt:
      - seccomp:unconfined
```

### entrypoint.sh (Base Container)
```bash
#!/bin/bash
set -e

echo "=== ROCm Strix Container ==="

if [ -z "$HSA_OVERRIDE_GFX_VERSION" ]; then
    export HSA_OVERRIDE_GFX_VERSION=11.5.1
fi

echo "[INFO] Checking GPU..."
python3 -c "
import torch
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')
    print(f'ROCm: {torch.version.hip}')
else:
    print('ERROR: GPU not detected')
    exit(1)
"

echo "[INFO] GPU check passed. Container ready."
exec "$@"
```

### ollama/Dockerfile
```dockerfile
# =============================================================================
# Ollama Service - ROCm with Auto-Model Download
# =============================================================================
# Base: Ollama ROCm image
# Features: Automatic model download on first start

FROM ollama/ollama:rocm

# Install dependencies for health checks and process management
RUN apt-get update && apt-get install -y --no-install-recommends \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Default model to auto-download (can be overridden)
ENV OLLAMA_MODEL=gpt-oss:20b \
    OLLAMA_HOST=0.0.0.0:11434 \
    OLLAMA_NUM_PARALLEL=1 \
    OLLAMA_MAX_LOADED_MODELS=1 \
    HSA_OVERRIDE_GFX_VERSION=11.5.1 \
    HIP_VISIBLE_DEVICES=0 \
    AMD_SERIALIZE_KERNEL=1

# Copy entrypoint script
COPY entrypoint.sh /usr/local/bin/entrypoint.sh
RUN chmod +x /usr/local/bin/entrypoint.sh

# Expose Ollama port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=5 \
    CMD ollama list > /dev/null 2>&1 || exit 1

# Use custom entrypoint
ENTRYPOINT ["/usr/local/bin/entrypoint.sh"]
```

### ollama/entrypoint.sh
```bash
#!/bin/bash
set -e

# =============================================================================
# Ollama Auto-Model Loader Entrypoint
# =============================================================================
# Responsibilities:
# 1. Start Ollama server
# 2. Wait for server readiness
# 3. Auto-download configured model if not present
# 4. Keep container running

MODEL_NAME="${OLLAMA_MODEL:-gpt-oss:20b}"
OLLAMA_PID=""

log() {
    echo "[ollama] $1"
}

# Cleanup function
cleanup() {
    log "Shutting down..."
    if [ -n "$OLLAMA_PID" ]; then
        kill "$OLLAMA_PID" 2>/dev/null || true
        wait "$OLLAMA_PID" 2>/dev/null || true
    fi
}
trap cleanup SIGTERM SIGINT

log "========================================"
log "Model: $MODEL_NAME"
log "========================================"

# Start Ollama server in background
log "Starting Ollama server..."
/bin/ollama serve &
OLLAMA_PID=$!

# Wait for server to be ready
log "Waiting for server to be ready..."
for i in {1..60}; do
    if timeout 2 bash -c 'cat < /dev/null > /dev/tcp/localhost/11434' 2>/dev/null; then
        log "✓ Server is ready"
        break
    fi
    sleep 1
    if [ $i -eq 60 ]; then
        log "✗ Timeout waiting for server"
        exit 1
    fi
done

sleep 2

# Check and download model if needed
log "Checking model: $MODEL_NAME..."
if /bin/ollama list 2>/dev/null | grep -q "$MODEL_NAME"; then
    log "✓ Model already exists"
else
    log "⊘ Model not found. Downloading (~13.8GB)..."
    /bin/ollama pull "$MODEL_NAME"
    log "✓ Download complete"
fi

# Verify
if /bin/ollama list 2>/dev/null | grep -q "$MODEL_NAME"; then
    log "✓ Model ready"
else
    log "✗ Model verification failed"
    exit 1
fi

log "========================================"
log "Ollama running with GPU support"
log "Model: $MODEL_NAME"
log "========================================"

# Wait for server process
wait $OLLAMA_PID
```

## Build & Run

### Base Container
```bash
# 1. Get your GIDs
getent group video | cut -d: -f3
getent group render | cut -d: -f3

# 2. Fill .env with numeric values (NOT "video" or "render")

# 3. Build and start
docker compose up -d --build

# 4. Verify GPU detection
docker logs rocm-strix
docker exec rocm-strix python -c "import torch; print(torch.cuda.get_device_name(0))"
```

### Ollama Service
```bash
cd ollama
docker build -t ollama-strix .
docker run -d --name ollama-strix \
  --privileged \
  --device /dev/kfd \
  --device /dev/dri \
  -p 11434:11434 \
  -e VIDEO_GID=44 \
  -e RENDER_GID=991 \
  -e HSA_OVERRIDE_GFX_VERSION=11.5.1 \
  -e HIP_VISIBLE_DEVICES=0 \
  ollama-strix
```

## Test Output (Working)

### Base Container
```
=== ROCm Strix Container ===
[INFO] Checking GPU...
GPU: Radeon 8060S Graphics
VRAM: 109.5 GB
ROCm: 7.2.53150-7b886380f9
[INFO] GPU check passed. Container ready.
```

### Ollama Container
```
[ollama] ========================================
[ollama] Model: gpt-oss:20b
[ollama] ========================================
[ollama] Starting Ollama server...
[ollama] Waiting for server to be ready...
[ollama] ✓ Server is ready
[ollama] Checking model: gpt-oss:20b...
[ollama] ✓ Model already exists
[ollama] ✓ Model ready
[ollama] ========================================
[ollama] Ollama running with GPU support
[ollama] Model: gpt-oss:20b
[ollama] ========================================
```

## Key Technical Details

- **Index URL**: https://rocm.prereleases.amd.com/whl/gfx1151/ (required for gfx1151)
- **HSA_OVERRIDE_GFX_VERSION**: Must be 11.5.1 for Strix Halo
- **group_add**: Requires numeric GIDs, string names fail silently
- **devices**: /dev/kfd and /dev/dri required for GPU access
- **privileged**: Required for proper GPU permissions
- **Ollama Port**: 11434 (API endpoint)
- **Default Model**: gpt-oss:20b (~13.8GB download)

## Verification Commands

### Base Container
```bash
# Inside container
python -c "import torch; print(torch.__version__)"  # 2.9.1+rocm7.11.0rc1
python -c "import torch; print(torch.version.hip)"  # 7.2.x (HIP runtime)
python -c "import torch; print(torch.cuda.is_available())"  # True
```

### Ollama Container
```bash
# Check Ollama API
curl http://localhost:11434/api/tags

# Generate text
curl http://localhost:11434/api/generate -d '{
  "model": "gpt-oss:20b",
  "prompt": "Hello!"
}'
```
